{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "adb0ccdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'doc1': 'kembang sistem informasi jadwal', 'doc2': 'kembang model analisis sentimen berita', 'doc3': 'analisis sistem input output', 'doc4': 'kembang sistem informasi akademik universitas', 'doc5': 'kembang sistem cari berita ekonomi', 'doc6': 'analisis sistem neraca nasional', 'doc7': 'kembang sistem informasi layan statistik', 'doc8': 'kembang sistem cari skripsi di universitas', 'doc9': 'analisis sentimen publik hadap perintah', 'doc10': 'kembang model klasifikasi sentimen berita'}\n"
     ]
    }
   ],
   "source": [
    "#function tokenisasi\n",
    "def tokenisasi(text):\n",
    "    tokens = text.split(\" \")\n",
    "    return tokens\n",
    "\n",
    "#function stemming\n",
    "def stemming(text):\n",
    "    from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "    # create stemmer\n",
    "    factory = StemmerFactory()\n",
    "    stemmer = factory.create_stemmer()\n",
    "    # stemming process\n",
    "    output = stemmer.stem(text)\n",
    "    return output\n",
    "\n",
    "#function stemming sentence\n",
    "def stemming_sentence(text):\n",
    "    output = \"\"\n",
    "    for token in tokenisasi(text):\n",
    "        output = output + stemming(token) + \" \"\n",
    "    return output[:-1]\n",
    "\n",
    "doc_dict_raw = {}\n",
    "doc_dict_raw['doc1']=\"pengembangan sistem informasi penjadwalan\"\n",
    "doc_dict_raw['doc2']=\"pengembangan model analisis sentimen berita\"\n",
    "doc_dict_raw['doc3'] = \"analisis sistem input output\"\n",
    "doc_dict_raw['doc4'] = \"pengembangan sistem informasi akademik universitas\"\n",
    "doc_dict_raw['doc5'] = \"pengembangan sistem cari berita ekonomi\"\n",
    "doc_dict_raw['doc6'] = \"analisis sistem neraca nasional\"\n",
    "doc_dict_raw['doc7'] = \"pengembangan sistem informasi layanan statistik\"\n",
    "doc_dict_raw['doc8'] = \"pengembangan sistem pencarian skripsi di universitas\"\n",
    "doc_dict_raw['doc9'] = \"analisis sentimen publik terhadap pemerintah\"\n",
    "doc_dict_raw['doc10'] = \"pengembangan model klasifikasi sentimen berita\"\n",
    "\n",
    "doc_dict ={}\n",
    "for doc_id,doc in doc_dict_raw.items():\n",
    "    doc_dict[doc_id]=stemming_sentence(doc)\n",
    "print(doc_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5ec5e8ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kembang model klasifikasi sentimen berita\n"
     ]
    }
   ],
   "source": [
    "vocab = []\n",
    "inverted_index= {}\n",
    "for doc_id, doc in doc_dict.items():\n",
    "    for token in tokenisasi(doc):\n",
    "        \n",
    "        if token not in vocab:\n",
    "            vocab.append(token)\n",
    "            inverted_index[token] = []\n",
    "        if token in inverted_index:\n",
    "            if doc_id not in inverted_index[token]:\n",
    "                inverted_index[token].append(doc_id)\n",
    "#print(vocab)\n",
    "\n",
    "print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9fd8073",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "print(len(doc_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf25e1d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'kembang': 0, 'sistem': 0, 'informasi': 0, 'jadwal': 0, 'model': 0, 'analisis': 0, 'sentimen': 0, 'berita': 0, 'input': 0, 'output': 0, 'akademik': 0, 'universitas': 1, 'cari': 0, 'ekonomi': 0, 'neraca': 0, 'nasional': 0, 'layan': 0, 'statistik': 0, 'skripsi': 0, 'di': 1, 'publik': 0, 'hadap': 0, 'perintah': 0, 'klasifikasi': 0}\n"
     ]
    }
   ],
   "source": [
    "query = \"Sistem pengembangan universitas di dalam kehidupan sehari-hari\"\n",
    "def termFrequency(vocab, query):\n",
    "    tf_query = {}\n",
    "    for word in vocab:\n",
    "        tf_query[word] = query.count(word)\n",
    "    return tf_query\n",
    "tf_query = termFrequency(vocab, query)\n",
    "print(tf_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "67d93502",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'kembang': 7, 'sistem': 7, 'informasi': 3, 'jadwal': 1, 'model': 2, 'analisis': 4, 'sentimen': 3, 'berita': 3, 'input': 1, 'output': 1, 'akademik': 1, 'universitas': 2, 'cari': 2, 'ekonomi': 1, 'neraca': 1, 'nasional': 1, 'layan': 1, 'statistik': 1, 'skripsi': 1, 'di': 1, 'publik': 1, 'hadap': 1, 'perintah': 1, 'klasifikasi': 1}\n",
      "{'kembang': 1.1383026981662814, 'sistem': 1.1383026981662814, 'informasi': 1.4393326938302626, 'jadwal': 1.7403626894942439, 'model': 1.5642714304385625, 'analisis': 1.3424226808222062, 'sentimen': 1.4393326938302626, 'berita': 1.4393326938302626, 'input': 1.7403626894942439, 'output': 1.7403626894942439, 'akademik': 1.7403626894942439, 'universitas': 1.5642714304385625, 'cari': 1.5642714304385625, 'ekonomi': 1.7403626894942439, 'neraca': 1.7403626894942439, 'nasional': 1.7403626894942439, 'layan': 1.7403626894942439, 'statistik': 1.7403626894942439, 'skripsi': 1.7403626894942439, 'di': 1.7403626894942439, 'publik': 1.7403626894942439, 'hadap': 1.7403626894942439, 'perintah': 1.7403626894942439, 'klasifikasi': 1.7403626894942439}\n",
      "[[0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [1.56427143]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [1.74036269]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]]\n",
      "[[1.1383027  1.1383027  0.         1.1383027  1.1383027  0.\n",
      "  1.1383027  1.1383027  0.         1.1383027 ]\n",
      " [1.1383027  0.         1.1383027  1.1383027  1.1383027  1.1383027\n",
      "  1.1383027  1.1383027  0.         0.        ]\n",
      " [1.43933269 0.         0.         1.43933269 0.         0.\n",
      "  1.43933269 0.         0.         0.        ]\n",
      " [1.74036269 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         1.56427143 0.         0.         0.         0.\n",
      "  0.         0.         0.         1.56427143]\n",
      " [0.         1.34242268 1.34242268 0.         0.         1.34242268\n",
      "  0.         0.         1.34242268 0.        ]\n",
      " [0.         1.43933269 0.         0.         0.         0.\n",
      "  0.         0.         1.43933269 1.43933269]\n",
      " [0.         1.43933269 0.         0.         1.43933269 0.\n",
      "  0.         0.         0.         1.43933269]\n",
      " [0.         0.         1.74036269 0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         1.74036269 0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         1.74036269 0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         1.56427143 0.         0.\n",
      "  0.         1.56427143 0.         0.        ]\n",
      " [0.         0.         0.         0.         1.56427143 0.\n",
      "  0.         1.56427143 0.         0.        ]\n",
      " [0.         0.         0.         0.         1.74036269 0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         1.74036269\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         1.74036269\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  1.74036269 0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  1.74036269 0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         1.74036269 0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         1.74036269 0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         1.74036269 0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         1.74036269 0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         1.74036269 0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         1.74036269]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "#menghitung df yang merupakan freq kata pada suatu dokumen \n",
    "def wordDocFre(vocab,doc_dict):\n",
    "    df={}\n",
    "    for word in vocab:\n",
    "        frq = 0\n",
    "        for doc in doc_dict.values():\n",
    "            if word in tokenisasi(doc):\n",
    "                frq=frq+1\n",
    "        df[word]=frq \n",
    "    return df\n",
    "\n",
    "print(wordDocFre(vocab,doc_dict))\n",
    "\n",
    "def inverseDocFre(vocab,doc_fre,length):\n",
    "    idf={}\n",
    "    for word in vocab:\n",
    "        idf[word]=idf[word]=1+np.log10((length+1)/(doc_fre[word]+1))\n",
    "    return idf\n",
    "\n",
    "print(inverseDocFre(vocab,wordDocFre(vocab,doc_dict),len(doc_dict)))\n",
    "idf = inverseDocFre(vocab,wordDocFre(vocab,doc_dict),len(doc_dict))\n",
    "\n",
    "# Term - Query Matrix\n",
    "TQ = np.zeros((len(vocab), 1)) #hanya 1 query\n",
    "length=len(doc_dict)\n",
    "#idf = inverseDocFre(vocab,doc_dict,length)\n",
    "for word in vocab:\n",
    "    ind1 = vocab.index(word)\n",
    "    TQ[ind1][0] = tf_query[word]*idf[word]\n",
    "print(TQ)\n",
    "\n",
    "def termFrequencyInDoc(vocab,doc_dict):\n",
    "    tf_docs = {}\n",
    "    for doc_id in doc_dict.keys():\n",
    "        tf_docs[doc_id]={}\n",
    "    \n",
    "    for word in vocab:\n",
    "        for doc_id, doc in doc_dict.items():\n",
    "            tf_docs[doc_id][word]=doc.count(word)\n",
    "    return tf_docs\n",
    "\n",
    "def tfidf(vocab,tf,idf_scr,doc_dict):\n",
    "    tf_idf_scr={}\n",
    "    for doc_id in doc_dict.keys():\n",
    "        tf_idf_scr[doc_id]={}\n",
    "    for word in vocab:\n",
    "        for doc_id, doc in doc_dict.items():\n",
    "            tf_idf_scr[doc_id][word] = tf[doc_id][word]*idf_scr[word]\n",
    "    return tf_idf_scr\n",
    "#tem document\n",
    "tf_idf = tfidf(vocab, termFrequencyInDoc(vocab,doc_dict),inverseDocFre(vocab,wordDocFre(vocab,doc_dict),len(doc_dict)),doc_dict)\n",
    "TD = np.zeros((len(vocab),len(doc_dict)))\n",
    "for word in vocab:\n",
    "    for doc_id, doc in tf_idf.items():\n",
    "        ind1=vocab.index(word)\n",
    "        ind2=list(tf_idf.keys()).index(doc_id)\n",
    "        TD[ind1][ind2]=tf_idf[doc_id][word]\n",
    "print(TD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cedbbedf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.328400348945259\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.635865623181743\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "def cosine_sim(vec1,vec2):\n",
    "    vec1=list(vec1)\n",
    "    vec2=list(vec2)\n",
    "    dot_prod=0\n",
    "    for i, v in enumerate(vec1):\n",
    "        dot_prod+=v*vec2[i]\n",
    "    mag_1=math.sqrt(sum([x**2 for x in vec1]))\n",
    "    mag_2=math.sqrt(sum([x**2 for x in vec2]))\n",
    "    return dot_prod/(mag_1 * mag_2)\n",
    "\n",
    "print(cosine_sim(TQ[:, 0], TD[:, 0])) #query & doc1\n",
    "print(cosine_sim(TQ[:, 0], TD[:, 1])) #query & doc2\n",
    "print(cosine_sim(TQ[:, 0], TD[:, 2])) #query & doc3\n",
    "print(cosine_sim(TQ[:, 0], TD[:, 3])) #query & doc3\n",
    "print(cosine_sim(TQ[:, 0], TD[:, 4])) #query & doc3\n",
    "print(cosine_sim(TQ[:, 0], TD[:, 5])) #query & doc3\n",
    "print(cosine_sim(TQ[:, 0], TD[:, 6])) #query & doc3\n",
    "print(cosine_sim(TQ[:, 0], TD[:, 7])) #query & doc3\n",
    "print(cosine_sim(TQ[:, 0], TD[:, 8])) #query & doc3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "31df51ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.1383027 , 1.1383027 , 1.43933269, 1.74036269, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        ])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TD[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b072cfd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'doc8': 0.635865623181743, 'doc4': 0.328400348945259}\n"
     ]
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "def exact_top_k(doc_dict, TD, q, k):\n",
    "    relevance_scores = {}\n",
    "    i = 0\n",
    "    for doc_id in doc_dict.keys():\n",
    "        relevance_scores[doc_id] = cosine_sim(q, TD[:, i])\n",
    "        i = i + 1\n",
    "    sorted_value = OrderedDict(sorted(relevance_scores.items(), key=lambda x: x[1], reverse = True))\n",
    "    top_k = {j: sorted_value[j] for j in list(sorted_value)[:k]}\n",
    "    return top_k\n",
    "top_2 = exact_top_k(doc_dict, TD, TQ[:, 0], 2)\n",
    "#misal k = 3\n",
    "top_3=exact_top_k(doc_dict,TD,TQ[:,0],3)\n",
    "top_2=exact_top_k(doc_dict,TD,TQ[:,0],2)\n",
    "print(top_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb0d7f7",
   "metadata": {},
   "source": [
    "# Index Elimination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c5ce4d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_elim_simple(query,doc_dict):\n",
    "    remove_list=[]\n",
    "    for doc_id, doc in doc_dict.items():\n",
    "        n=0\n",
    "        for word in tokenisasi(query):\n",
    "            if stemming(word) in doc:\n",
    "                n=n+1\n",
    "        if n==0:\n",
    "                remove_list.append(doc_id)\n",
    "    for key in remove_list:\n",
    "            del doc_dict[key]\n",
    "    return doc_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "46b4e6a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "query =\"sistem informasi statistik\"\n",
    "doc_dict=index_elim_simple(query,doc_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1a5ffed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def elim_query(query,idf_dict, idf_score):\n",
    "    for term in tokenisasi(query):\n",
    "        if idf_dict[stemming(term)]<idf_score:\n",
    "            query=query.replace(term+\"\",\"\")\n",
    "            query=query.replace(term,\"\")\n",
    "    return query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4fe59398",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  statistik\n"
     ]
    }
   ],
   "source": [
    "query=\"sistem informasi statistik\"\n",
    "query=elim_query(query,idf,1.5)\n",
    "print(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d702a1d",
   "metadata": {},
   "source": [
    "# Champion List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "47228a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_championlist(inverted_index,tf_idf,r):\n",
    "    champion_list={}\n",
    "    for term in inverted_index.keys():\n",
    "        weight_scores={}\n",
    "        for doc_id,tf in tf_idf.items():\n",
    "            if tf_idf[doc_id][term]!=0:\n",
    "                weight_scores[doc_id] = tf_idf[doc_id][term]\n",
    "        sorted_value= OrderedDict(sorted(weight_scores.items(),key=lambda x:x[1],reverse=True))\n",
    "        top_r={j: sorted_value[j] for j in list(sorted_value)[:r]}\n",
    "        champion_list[term]=list(top_r.keys())\n",
    "    return champion_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5d911a",
   "metadata": {},
   "source": [
    "Kemudian panggil fungsi di atas untuk mendapatkan champion list untuk r tertentu, misalnya r=2.\n",
    "Bandingkan isi champion list dan inverted index yang telah dibuat sebelumnya."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cb68ded3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'kembang': ['doc1', 'doc2'],\n",
       " 'sistem': ['doc1', 'doc3'],\n",
       " 'informasi': ['doc1', 'doc4'],\n",
       " 'jadwal': ['doc1'],\n",
       " 'model': ['doc2', 'doc10'],\n",
       " 'analisis': ['doc2', 'doc3'],\n",
       " 'sentimen': ['doc2', 'doc9'],\n",
       " 'berita': ['doc2', 'doc5'],\n",
       " 'input': ['doc3'],\n",
       " 'output': ['doc3'],\n",
       " 'akademik': ['doc4'],\n",
       " 'universitas': ['doc4', 'doc8'],\n",
       " 'cari': ['doc5', 'doc8'],\n",
       " 'ekonomi': ['doc5'],\n",
       " 'neraca': ['doc6'],\n",
       " 'nasional': ['doc6'],\n",
       " 'layan': ['doc7'],\n",
       " 'statistik': ['doc7'],\n",
       " 'skripsi': ['doc8'],\n",
       " 'di': ['doc8'],\n",
       " 'publik': ['doc9'],\n",
       " 'hadap': ['doc9'],\n",
       " 'perintah': ['doc9'],\n",
       " 'klasifikasi': ['doc10']}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r=2\n",
    "create_championlist(inverted_index,tf_idf,r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60954d8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7f9dad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
